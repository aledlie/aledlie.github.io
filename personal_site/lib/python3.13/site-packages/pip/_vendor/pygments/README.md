# pygments

## Overview

This directory contains 14 code file(s) with extracted schemas.

## Subdirectories

- `filters/`
- `formatters/`
- `lexers/`
- `styles/`

## Files and Schemas

### `__init__.py` (python)

**Functions:**
- `lex(code, lexer)` - Line 35
- `format(tokens, formatter, outfile)` - Line 52
- `highlight(code, lexer, formatter, outfile)` - Line 77

**Key Imports:** `io`, `pip._vendor.pygments.formatter`, `pip._vendor.pygments.lexer`

### `console.py` (python)

**Functions:**
- `reset_color()` - Line 40
- `colorize(color_key, text)` - Line 44
- `ansiformat(attr, text)` - Line 48

### `filter.py` (python)

**Classes:**
- `Filter` - Line 41
  - Default filter. Subclass this class or use the `simplefilter`
  - Methods: __init__, filter
- `FunctionFilter` (extends: Filter) - Line 54
  - Abstract class used by `simplefilter` to create simple
  - Methods: __init__, filter

**Functions:**
- `apply_filters(stream, filters, lexer)` - Line 12
- `simplefilter(f)` - Line 25

### `formatter.py` (python)

**Classes:**
- `Formatter` - Line 25
  - Converts a token stream to text.
  - Methods: __init__, get_style_defs, format, __class_getitem__

**Functions:**
- `_lookup_style(style)` - Line 19

**Key Imports:** `codecs`, `pip._vendor.pygments.styles`, `pip._vendor.pygments.util`

### `lexer.py` (python)

**Classes:**
- `LexerMeta` (extends: type) - Line 37
  - This metaclass automagically converts ``analyse_text`` methods into
  - Methods: __new__
- `Lexer` - Line 49
  - Lexer for a specific language.
  - Methods: __init__, __repr__, add_filter, analyse_text, _preprocess_lexer_input (+2 more)
- `DelegatingLexer` (extends: Lexer) - Line 291
  - This lexer takes two lexer as arguments. A root lexer and
  - Methods: __init__, get_tokens_unprocessed
- `include` (extends: str) - Line 330
  - Indicates that a state should include rules from another state.
- `_inherit` - Line 337
  - Indicates the a state should inherit from its superclass.
  - Methods: __repr__
- `combined` (extends: tuple) - Line 347
  - Indicates a state combined from multiple states.
  - Methods: __new__, __init__
- `_PseudoMatch` - Line 360
  - A pseudo match object constructed from a string.
  - Methods: __init__, start, end, group, groups (+1 more)
- `_This` - Line 413
  - Special singleton used for indicating the caller class.
- `default` - Line 472
  - Indicates a state or state action (e.g. #pop) to apply.
  - Methods: __init__
- `words` (extends: Future) - Line 484
  - Indicates a list of literal words that is transformed into an optimized
  - Methods: __init__, get
- `RegexLexerMeta` (extends: LexerMeta) - Line 500
  - Metaclass for RegexLexer, creates the self._tokens attribute from
  - Methods: _process_regex, _process_token, _process_new_state, _process_state, process_tokendef (+2 more)
- `RegexLexer` (extends: Lexer) - Line 667
  - Base for simple stateful regular expression-based lexers.
  - Methods: get_tokens_unprocessed
- `LexerContext` - Line 764
  - A helper object that holds lexer position data.
  - Methods: __init__, __repr__
- `ExtendedRegexLexer` (extends: RegexLexer) - Line 779
  - A RegexLexer that uses a context object to store its state.
  - Methods: get_tokens_unprocessed
- `ProfilingRegexLexerMeta` (extends: RegexLexerMeta) - Line 915
  - Metaclass for ProfilingRegexLexer, collects regex timing info.
  - Methods: _process_regex
- `ProfilingRegexLexer` (extends: RegexLexer) - Line 937
  - Drop-in replacement for RegexLexer that does profiling of its regexes.
  - Methods: get_tokens_unprocessed

**Functions:**
- `bygroups()` - Line 387
- `using(_other)` - Line 422
- `do_insertions(insertions, tokens)` - Line 851

**Key Imports:** `pip._vendor.pygments.filter`, `pip._vendor.pygments.filters`, `pip._vendor.pygments.regexopt`, `pip._vendor.pygments.token`, `pip._vendor.pygments.util` (+3 more)

### `modeline.py` (python)

**Functions:**
- `get_filetype_from_line(l)` - Line 22
- `get_filetype_from_buffer(buf, max_lines)` - Line 28

**Key Imports:** `re`

### `plugin.py` (python)

**Functions:**
- `iter_entry_points(group_name)` - Line 43
- `find_plugin_lexers()` - Line 55
- `find_plugin_formatters()` - Line 60
- `find_plugin_styles()` - Line 65
- `find_plugin_filters()` - Line 70

**Key Imports:** `importlib.metadata`

### `regexopt.py` (python)

**Functions:**
- `make_charset(letters)` - Line 22
- `regex_opt_inner(strings, open_paren)` - Line 26
- `regex_opt(strings, prefix, suffix)` - Line 82

**Key Imports:** `itertools`, `operator`, `os.path`, `re`

### `scanner.py` (python)

**Classes:**
- `EndOfText` (extends: RuntimeError) - Line 20
  - Raise if end of text is reached and the user
- `Scanner` - Line 27
  - Simple scanner
  - Methods: __init__, eos, check, test, scan (+2 more)

**Key Imports:** `re`

### `sphinxext.py` (python)

**Classes:**
- `PygmentsDoc` (extends: Directive) - Line 60
  - A directive to collect all lexers/formatters/filters and generate
  - Methods: run, document_lexers_overview, document_lexers, document_formatters, document_filters

**Functions:**
- `setup(app)` - Line 246

**Key Imports:** `docutils`, `docutils.parsers.rst`, `docutils.statemachine`, `inspect`, `pathlib` (+7 more)

### `style.py` (python)

**Classes:**
- `StyleMeta` (extends: type) - Line 58
  - Methods: __new__, style_for_token, list_styles, styles_token, __iter__ (+1 more)
- `Style` - Line 170

**Key Imports:** `pip._vendor.pygments.token`

### `token.py` (python)

**Classes:**
- `_TokenType` (extends: tuple) - Line 12
  - Methods: split, __init__, __contains__, __getattr__, __repr__ (+2 more)

**Functions:**
- `is_token_subtype(ttype, other)` - Line 85
- `string_to_tokentype(s)` - Line 94

### `unistring.py` (python)

**Functions:**
- `combine()` - Line 82
- `allexcept()` - Line 86
- `_handle_runs(char_list)` - Line 93

**Key Imports:** `unicodedata`

### `util.py` (python)

**Classes:**
- `ClassNotFound` (extends: ValueError) - Line 30
  - Raised if one of the lookup functions didn't find a matching class.
- `OptionError` (extends: Exception) - Line 34
  - This exception will be raised by all option processing functions if
- `Future` - Line 265
  - Generic class to defer some work.
  - Methods: get
- `UnclosingTextIOWrapper` (extends: TextIOWrapper) - Line 321
  - Methods: close

**Functions:**
- `get_choice_opt(options, optname, allowed, default, normcase)` - Line 40
- `get_bool_opt(options, optname, default)` - Line 53
- `get_int_opt(options, optname, default)` - Line 82
- `get_list_opt(options, optname, default)` - Line 94
- `docstring_headline(obj)` - Line 110
- `make_analysator(f)` - Line 122
- `shebang_matches(text, regex)` - Line 139
- `doctype_matches(text, regex)` - Line 184
- `html_doctype_matches(text)` - Line 197
- `looks_like_xml(text)` - Line 205
- ... and 6 more functions

**Key Imports:** `io`, `locale`, `re`

---
*Generated by Schema Generator*